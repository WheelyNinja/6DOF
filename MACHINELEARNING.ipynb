{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4017250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "493fbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "with open('cases_R2.csv','r') as csvfile:\n",
    "    read = csv.reader(csvfile, delimiter = ',')\n",
    "    for x in read:\n",
    "        X.append(x)\n",
    "\n",
    "\n",
    "X_train = []\n",
    "for i in X:\n",
    "    X_train.append([float(j) for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb46bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "with open('label2.csv','r') as csvfile:\n",
    "    read = csv.reader(csvfile, delimiter = ',')\n",
    "    for y in read:\n",
    "        Y.append(y)\n",
    "\n",
    "y_train = []\n",
    "for i in range(len(Y)):\n",
    "    for i in Y[i]:\n",
    "        y_train.append(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb79311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split (X_train, y_train, test_size=0, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92d06db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1230.91581263\n",
      "Iteration 2, loss = 1000.30730014\n",
      "Iteration 3, loss = 933.39136707\n",
      "Iteration 4, loss = 889.00353873\n",
      "Iteration 5, loss = 847.67724641\n",
      "Iteration 6, loss = 818.59118194\n",
      "Iteration 7, loss = 789.72896061\n",
      "Iteration 8, loss = 766.68546905\n",
      "Iteration 9, loss = 755.34195638\n",
      "Iteration 10, loss = 737.17708957\n",
      "Iteration 11, loss = 720.22432292\n",
      "Iteration 12, loss = 699.43984426\n",
      "Iteration 13, loss = 690.45823259\n",
      "Iteration 14, loss = 673.01718872\n",
      "Iteration 15, loss = 656.22006992\n",
      "Iteration 16, loss = 656.46513385\n",
      "Iteration 17, loss = 652.35264187\n",
      "Iteration 18, loss = 642.10431123\n",
      "Iteration 19, loss = 627.14092272\n",
      "Iteration 20, loss = 628.09955609\n",
      "Iteration 21, loss = 616.36119265\n",
      "Iteration 22, loss = 607.56207117\n",
      "Iteration 23, loss = 592.22889813\n",
      "Iteration 24, loss = 600.51377934\n",
      "Iteration 25, loss = 596.58352841\n",
      "Iteration 26, loss = 578.93749657\n",
      "Iteration 27, loss = 577.78399146\n",
      "Iteration 28, loss = 559.36296838\n",
      "Iteration 29, loss = 560.36805099\n",
      "Iteration 30, loss = 549.75914677\n",
      "Iteration 31, loss = 542.63865248\n",
      "Iteration 32, loss = 539.46207800\n",
      "Iteration 33, loss = 536.62762396\n",
      "Iteration 34, loss = 529.76016048\n",
      "Iteration 35, loss = 525.29016549\n",
      "Iteration 36, loss = 508.14443513\n",
      "Iteration 37, loss = 501.25298563\n",
      "Iteration 38, loss = 504.84372353\n",
      "Iteration 39, loss = 504.49473171\n",
      "Iteration 40, loss = 496.21955636\n",
      "Iteration 41, loss = 493.56502299\n",
      "Iteration 42, loss = 484.43456655\n",
      "Iteration 43, loss = 474.07474531\n",
      "Iteration 44, loss = 477.72735744\n",
      "Iteration 45, loss = 467.42839507\n",
      "Iteration 46, loss = 468.22075397\n",
      "Iteration 47, loss = 460.58016400\n",
      "Iteration 48, loss = 459.35950249\n",
      "Iteration 49, loss = 462.64284789\n",
      "Iteration 50, loss = 456.12125397\n",
      "Iteration 51, loss = 450.65076094\n",
      "Iteration 52, loss = 453.06640570\n",
      "Iteration 53, loss = 444.32640375\n",
      "Iteration 54, loss = 440.74788269\n",
      "Iteration 55, loss = 436.07412135\n",
      "Iteration 56, loss = 434.25651068\n",
      "Iteration 57, loss = 431.16291793\n",
      "Iteration 58, loss = 434.97750953\n",
      "Iteration 59, loss = 427.87968379\n",
      "Iteration 60, loss = 425.32528728\n",
      "Iteration 61, loss = 426.99644652\n",
      "Iteration 62, loss = 420.37083522\n",
      "Iteration 63, loss = 415.62029556\n",
      "Iteration 64, loss = 412.62558334\n",
      "Iteration 65, loss = 413.80581662\n",
      "Iteration 66, loss = 400.02097682\n",
      "Iteration 67, loss = 392.40921527\n",
      "Iteration 68, loss = 394.54241991\n",
      "Iteration 69, loss = 389.70640765\n",
      "Iteration 70, loss = 390.91402200\n",
      "Iteration 71, loss = 395.39056998\n",
      "Iteration 72, loss = 384.26548652\n",
      "Iteration 73, loss = 384.15165114\n",
      "Iteration 74, loss = 379.64781687\n",
      "Iteration 75, loss = 367.83479117\n",
      "Iteration 76, loss = 364.36877047\n",
      "Iteration 77, loss = 360.94000083\n",
      "Iteration 78, loss = 357.87379779\n",
      "Iteration 79, loss = 354.91650944\n",
      "Iteration 80, loss = 353.57013723\n",
      "Iteration 81, loss = 348.74526835\n",
      "Iteration 82, loss = 356.22670206\n",
      "Iteration 83, loss = 344.42759539\n",
      "Iteration 84, loss = 348.97639781\n",
      "Iteration 85, loss = 340.96908395\n",
      "Iteration 86, loss = 340.64870427\n",
      "Iteration 87, loss = 341.26737731\n",
      "Iteration 88, loss = 336.96201819\n",
      "Iteration 89, loss = 331.70453628\n",
      "Iteration 90, loss = 331.87744621\n",
      "Iteration 91, loss = 337.55739949\n",
      "Iteration 92, loss = 328.75983812\n",
      "Iteration 93, loss = 336.16881075\n",
      "Iteration 94, loss = 331.98716012\n",
      "Iteration 95, loss = 338.14790590\n",
      "Iteration 96, loss = 340.04223732\n",
      "Iteration 97, loss = 330.01928543\n",
      "Iteration 98, loss = 319.94866643\n",
      "Iteration 99, loss = 319.40962679\n",
      "Iteration 100, loss = 313.57261734\n",
      "Iteration 101, loss = 314.82487298\n",
      "Iteration 102, loss = 311.89201060\n",
      "Iteration 103, loss = 305.50572730\n",
      "Iteration 104, loss = 309.66762226\n",
      "Iteration 105, loss = 299.25556109\n",
      "Iteration 106, loss = 294.92129237\n",
      "Iteration 107, loss = 297.22830770\n",
      "Iteration 108, loss = 292.12014256\n",
      "Iteration 109, loss = 298.27823886\n",
      "Iteration 110, loss = 304.57922819\n",
      "Iteration 111, loss = 292.73457652\n",
      "Iteration 112, loss = 294.16650633\n",
      "Iteration 113, loss = 288.29438124\n",
      "Iteration 114, loss = 286.13361165\n",
      "Iteration 115, loss = 286.55506008\n",
      "Iteration 116, loss = 281.50606319\n",
      "Iteration 117, loss = 285.51151970\n",
      "Iteration 118, loss = 280.21206613\n",
      "Iteration 119, loss = 280.60037900\n",
      "Iteration 120, loss = 278.92843578\n",
      "Iteration 121, loss = 281.57451816\n",
      "Iteration 122, loss = 276.52375838\n",
      "Iteration 123, loss = 274.95845655\n",
      "Iteration 124, loss = 272.43712371\n",
      "Iteration 125, loss = 277.47586760\n",
      "Iteration 126, loss = 284.67786592\n",
      "Iteration 127, loss = 275.57643629\n",
      "Iteration 128, loss = 270.51928494\n",
      "Iteration 129, loss = 270.88807172\n",
      "Iteration 130, loss = 263.28361541\n",
      "Iteration 131, loss = 270.31093904\n",
      "Iteration 132, loss = 266.83354665\n",
      "Iteration 133, loss = 275.70028174\n",
      "Iteration 134, loss = 272.12472131\n",
      "Iteration 135, loss = 275.92715497\n",
      "Iteration 136, loss = 268.46025062\n",
      "Iteration 137, loss = 264.66863941\n",
      "Iteration 138, loss = 260.43253564\n",
      "Iteration 139, loss = 255.13408110\n",
      "Iteration 140, loss = 252.95795145\n",
      "Iteration 141, loss = 244.99896677\n",
      "Iteration 142, loss = 250.22338211\n",
      "Iteration 143, loss = 245.57030356\n",
      "Iteration 144, loss = 251.66919164\n",
      "Iteration 145, loss = 250.68081283\n",
      "Iteration 146, loss = 250.72463761\n",
      "Iteration 147, loss = 244.48317883\n",
      "Iteration 148, loss = 248.86417391\n",
      "Iteration 149, loss = 244.76179380\n",
      "Iteration 150, loss = 240.81415320\n",
      "Iteration 151, loss = 242.47336071\n",
      "Iteration 152, loss = 240.28734854\n",
      "Iteration 153, loss = 241.26924537\n",
      "Iteration 154, loss = 238.51218166\n",
      "Iteration 155, loss = 242.73481474\n",
      "Iteration 156, loss = 239.84182537\n",
      "Iteration 157, loss = 239.81206889\n",
      "Iteration 158, loss = 237.02855403\n",
      "Iteration 159, loss = 234.07121262\n",
      "Iteration 160, loss = 236.19860764\n",
      "Iteration 161, loss = 240.13288494\n",
      "Iteration 162, loss = 237.27767693\n",
      "Iteration 163, loss = 235.10085258\n",
      "Iteration 164, loss = 234.29971866\n",
      "Iteration 165, loss = 231.94761524\n",
      "Iteration 166, loss = 229.00872678\n",
      "Iteration 167, loss = 224.21721406\n",
      "Iteration 168, loss = 223.70263527\n",
      "Iteration 169, loss = 225.44880078\n",
      "Iteration 170, loss = 220.97408194\n",
      "Iteration 171, loss = 217.85825980\n",
      "Iteration 172, loss = 217.02766686\n",
      "Iteration 173, loss = 217.97615044\n",
      "Iteration 174, loss = 215.34417442\n",
      "Iteration 175, loss = 215.61218220\n",
      "Iteration 176, loss = 214.85501087\n",
      "Iteration 177, loss = 211.92890816\n",
      "Iteration 178, loss = 223.98666108\n",
      "Iteration 179, loss = 219.12606818\n",
      "Iteration 180, loss = 214.56695003\n",
      "Iteration 181, loss = 211.74428913\n",
      "Iteration 182, loss = 219.86824253\n",
      "Iteration 183, loss = 226.52998596\n",
      "Iteration 184, loss = 222.61421449\n",
      "Iteration 185, loss = 216.61654699\n",
      "Iteration 186, loss = 217.93721228\n",
      "Iteration 187, loss = 213.49796064\n",
      "Iteration 188, loss = 210.77186858\n",
      "Iteration 189, loss = 214.00712291\n",
      "Iteration 190, loss = 208.44743636\n",
      "Iteration 191, loss = 211.29837606\n",
      "Iteration 192, loss = 212.87432034\n",
      "Iteration 193, loss = 213.82063165\n",
      "Iteration 194, loss = 205.41665592\n",
      "Iteration 195, loss = 202.19713868\n",
      "Iteration 196, loss = 203.03453708\n",
      "Iteration 197, loss = 198.72523169\n",
      "Iteration 198, loss = 204.73353584\n",
      "Iteration 199, loss = 209.79517093\n",
      "Iteration 200, loss = 207.24864280\n",
      "Iteration 201, loss = 203.91128635\n",
      "Iteration 202, loss = 203.69923773\n",
      "Iteration 203, loss = 202.60969123\n",
      "Iteration 204, loss = 198.18790940\n",
      "Iteration 205, loss = 199.65953972\n",
      "Iteration 206, loss = 200.92201971\n",
      "Iteration 207, loss = 201.23197765\n",
      "Iteration 208, loss = 196.03885859\n",
      "Iteration 209, loss = 193.06739487\n",
      "Iteration 210, loss = 195.09457713\n",
      "Iteration 211, loss = 198.81684724\n",
      "Iteration 212, loss = 192.94749499\n",
      "Iteration 213, loss = 192.98615657\n",
      "Iteration 214, loss = 194.53651131\n",
      "Iteration 215, loss = 196.52598324\n",
      "Iteration 216, loss = 196.25145611\n",
      "Iteration 217, loss = 198.16374065\n",
      "Iteration 218, loss = 189.58991279\n",
      "Iteration 219, loss = 192.27271378\n",
      "Iteration 220, loss = 189.79144579\n",
      "Iteration 221, loss = 192.99608758\n",
      "Iteration 222, loss = 193.06584836\n",
      "Iteration 223, loss = 191.98938092\n",
      "Iteration 224, loss = 193.39090503\n",
      "Iteration 225, loss = 192.64950886\n",
      "Iteration 226, loss = 199.83674275\n",
      "Iteration 227, loss = 188.40813876\n",
      "Iteration 228, loss = 191.95372665\n",
      "Iteration 229, loss = 191.91019797\n",
      "Iteration 230, loss = 192.81713971\n",
      "Iteration 231, loss = 191.09547619\n",
      "Iteration 232, loss = 196.05734672\n",
      "Iteration 233, loss = 191.34559846\n",
      "Iteration 234, loss = 191.52211819\n",
      "Iteration 235, loss = 192.54296353\n",
      "Iteration 236, loss = 192.46291650\n",
      "Iteration 237, loss = 189.58782319\n",
      "Iteration 238, loss = 186.32693448\n",
      "Iteration 239, loss = 187.29505597\n",
      "Iteration 240, loss = 186.43901735\n",
      "Iteration 241, loss = 194.49256478\n",
      "Iteration 242, loss = 189.59242176\n",
      "Iteration 243, loss = 188.38633544\n",
      "Iteration 244, loss = 192.03326493\n",
      "Iteration 245, loss = 193.14159081\n",
      "Iteration 246, loss = 192.94410184\n",
      "Iteration 247, loss = 186.64054359\n",
      "Iteration 248, loss = 186.57423035\n",
      "Iteration 249, loss = 185.70517106\n",
      "Iteration 250, loss = 188.75711870\n",
      "Iteration 251, loss = 189.80878873\n",
      "Iteration 252, loss = 181.98980658\n",
      "Iteration 253, loss = 182.15630270\n",
      "Iteration 254, loss = 185.29725238\n",
      "Iteration 255, loss = 188.49789777\n",
      "Iteration 256, loss = 183.93340854\n",
      "Iteration 257, loss = 182.00642719\n",
      "Iteration 258, loss = 178.26580517\n",
      "Iteration 259, loss = 176.67785131\n",
      "Iteration 260, loss = 175.26794560\n",
      "Iteration 261, loss = 172.67602773\n",
      "Iteration 262, loss = 175.04589012\n",
      "Iteration 263, loss = 171.48604570\n",
      "Iteration 264, loss = 173.46067871\n",
      "Iteration 265, loss = 176.08877884\n",
      "Iteration 266, loss = 177.63451775\n",
      "Iteration 267, loss = 175.81676177\n",
      "Iteration 268, loss = 180.19930658\n",
      "Iteration 269, loss = 181.64396032\n",
      "Iteration 270, loss = 182.85469205\n",
      "Iteration 271, loss = 182.41806682\n",
      "Iteration 272, loss = 175.46936232\n",
      "Iteration 273, loss = 177.58343190\n",
      "Iteration 274, loss = 176.05593068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MLP = MLPRegressor(max_iter = 10000, learning_rate_init = 0.005, hidden_layer_sizes = 2222, activation = 'tanh', verbose = True , random_state = 1)\n",
    "MLP = MLP.fit(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5955e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('model_pickle', 'wb') as f:\n",
    "    pickle.dump(MLP, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e37859",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = pickle.load(open(\"model_pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab31a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-36.70211483]\n"
     ]
    }
   ],
   "source": [
    "array = [X_train[4]]\n",
    "y_pred = MLP.predict(array)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b98d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded08ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
